{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Deep Learning**\n",
        "* It is used for dealing with Unstructered Data and Images.\n",
        "* It is purely replication of Human Brain.\n",
        "* Neural Netwrok is a perceptron.\n",
        "* *ANN(Artificial Neural Network)*\n",
        "\n",
        "  * It is a computing system inspired by analogy od biological neurons which used to process info as a human brain.\n",
        "  * Uses Feed Forward Network.\n",
        "  * It consists of Input layer, Hidden Layer and Output layer.\n",
        "  * Which category as higher proabbility it belongs to that category.\n",
        "\n",
        "* Steps:\n",
        "  1. Initialise weights near to zero, input are multiplied with resp to weights and sum(sigma(WiXi)) taken, this is passed through activation function to get resultant output.\n",
        "  2. Sum passed through activation function.\n",
        "  3. The Activation output is considered as resultant output.\n",
        "\n",
        "* Weight : How much that particular feature important for output.\n",
        "* Types of Activation Function:\n",
        "\n",
        "  1. Threshold :\n",
        "    * f(x) = 1 if x>=0 |\n",
        "      f(x) = 0 if x<0\n",
        "    * Shape _|-\n",
        "  2. Relu(Rectified Linear Unit):\n",
        "    * f(x) = max(x,0)\n",
        "    * Computationally Efficient\n",
        "    * Mostly used in Hidden Layers\n",
        "    * Shape _/\n",
        "  3. Sigmoid :\n",
        "    * f(x) = 1/(1+e^-x)\n",
        "    * S-Shaped curve\n",
        "    * Output range 0 to 1\n",
        "    * f(x)<0.5 deactivate | f(x)>0.5 activated\n",
        "    * Best for Classification Problems\n",
        "  4. Tanh\n",
        "    * f(x) = (1-e^-2x)/(1+e^-2x)\n",
        "    * S-Shaped\n",
        "    * Output range -1 to 1\n",
        "    * Optimization good\n",
        "  5. Softmax :\n",
        "    * Similar to Sigmoid\n",
        "    * if classes more than 2\n",
        "    * Returns probabilities highest choosen\n",
        "  6. E Relu:\n",
        "    * f(x) = @(e^x-1) if x<=0 | f(x) = x if x>0\n",
        "* Error(Loss Function) = 1/2(y-y^)^2 --> MSE(Mean Square Error)\n",
        "* For one record we call error as Loss Fucntion & for multiple we call as Cost Function.\n",
        "* Optimisers : Used to decrease Error\n",
        "  * Gradient Descent -> Update weights using back propagation until loss function is min.\n",
        "* Weight Updation Formula:\n",
        "  * W` = W - a($Error/$W)\n",
        "  * NewWeight = Old Weight - LearningRate*DerivativeOfError.w.r.t.Weight(Slope)\n",
        "* +ve slope -> right hand of line pointing upwards.\n",
        "* -ve slope -> right hand of line pointing upwards.\n",
        "* Graph between Lossfunction and Weights is Parabola.\n",
        "* If we have n data points if we consider:\n",
        "  * 1 -> Stochastic Gradient Descent\n",
        "  * k(k<n) -> Mini Batch Gradient Descent(Most Used)\n",
        "* Adagrad Optmiser:\n",
        " * Learning rate is Adaptive\n",
        " * n` = n/sqrt(@+e)  here @(alpha) = square(error)\n",
        "* AdaDelta Optimiser:\n",
        " * here @ is exponential\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g8wiOerxlOWI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ziqEgQdrlRfj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}